{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "776c1d0ee74fc5508a4a91ec4f3556e107b8b455"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r\"C:\\Users\\Anugrah\\Desktop\\MY PC\\Python\\Restaurant_Reviews.tsv\", delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7d73a4db8fe837094ff5e40c04919bf6047d1f3c"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Anugrah/nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Anugrah/nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m review \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     10\u001b[0m ps \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 11\u001b[0m review \u001b[38;5;241m=\u001b[39m [ps\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m     12\u001b[0m review \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(review)\n\u001b[0;32m     13\u001b[0m corpus\u001b[38;5;241m.\u001b[39mappend(review)\n",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m review \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     10\u001b[0m ps \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 11\u001b[0m review \u001b[38;5;241m=\u001b[39m [ps\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m review \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m     12\u001b[0m review \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(review)\n\u001b[0;32m     13\u001b[0m corpus\u001b[38;5;241m.\u001b[39mappend(review)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\env1\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Anugrah/nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\.conda\\\\envs\\\\env1\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Anugrah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4e5cc79ee441495e83613276c7dab35f0ac1ddd2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an instance of TfidfVectorizer with desired settings\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1500)\n",
    "\n",
    "# Fit and transform your text data into TF-IDF features\n",
    "X = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# Prepare your target variable\n",
    "y = dataset.iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2b105697aba2280f3e9d8f9f691c63aa31e5a38d"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[114  38]\n",
      " [ 40 108]]\n",
      "\n",
      "\n",
      "Accuracy is  74.0 %\n",
      "Precision is  0.74\n",
      "Recall is  0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.1)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "score1 = accuracy_score(y_test, y_pred)\n",
    "score2 = precision_score(y_test, y_pred)\n",
    "score3 = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \", round(score1 * 100, 2), \"%\")\n",
    "print(\"Precision is \", round(score2, 2))\n",
    "print(\"Recall is \", round(score3, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "3afba49eb67124353842111eec8a856b8b910822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[115  37]\n",
      " [ 32 116]]\n",
      "\n",
      "\n",
      "Accuracy is  77.0 %\n",
      "Precision is  0.76\n",
      "Recall is  0.78\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli NB\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB(alpha=0.8)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (\"Confusion Matrix:\\n\",cm)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "score1 = accuracy_score(y_test,y_pred)\n",
    "score2 = precision_score(y_test,y_pred)\n",
    "score3= recall_score(y_test,y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
    "print(\"Precision is \",round(score2,2))\n",
    "print(\"Recall is \",round(score3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy is  74.33 %\n",
      "Precision is  0.83\n",
      "Recall is  0.61\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "score1 = accuracy_score(y_test, y_pred)\n",
    "score2 = precision_score(y_test, y_pred)\n",
    "score3 = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \", round(score1 * 100, 2), \"%\")\n",
    "print(\"Precision is \", round(score2, 2))\n",
    "print(\"Recall is \", round(score3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy is  73.33 %\n",
      "Precision is  0.79\n",
      "Recall is  0.62\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create an XGBoost classifier instance\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "score1 = accuracy_score(y_test, y_pred)\n",
    "score2 = precision_score(y_test, y_pred)\n",
    "score3 = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \", round(score1 * 100, 2), \"%\")\n",
    "print(\"Precision is \", round(score2, 2))\n",
    "print(\"Recall is \", round(score3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy is  76.67 %\n",
      "Precision is  0.8\n",
      "Recall is  0.7\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier instance\n",
    "classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "score1 = accuracy_score(y_test,y_pred)\n",
    "score2 = precision_score(y_test,y_pred)\n",
    "score3= recall_score(y_test,y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
    "print(\"Precision is \",round(score2,2))\n",
    "print(\"Recall is \",round(score3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ec65c31586cbcd91c8ece17895b067dd23e76adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[125  27]\n",
      " [ 43 105]]\n",
      "\n",
      "\n",
      "Accuracy is  76.67 %\n",
      "Precision is  0.8\n",
      "Recall is  0.71\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn import linear_model\n",
    "classifier = linear_model.LogisticRegression(C=1.5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (\"Confusion Matrix:\\n\",cm)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "score1 = accuracy_score(y_test,y_pred)\n",
    "score2 = precision_score(y_test,y_pred)\n",
    "score3= recall_score(y_test,y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
    "print(\"Precision is \",round(score2,2))\n",
    "print(\"Recall is \",round(score3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b4ab0c5a0882e69b332bb3c607fe620e972942d"
   },
   "source": [
    "### Analysis and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6be3a04b43930fc0da789bdf38de7ce25fa477e1"
   },
   "source": [
    "In this study, an attempt has been made to classify sentiment analysis for restaurant reviews using machine learning techniques. Two algorithms namely Multinomial Naive Bayes and Bernoulli Naive Bayes are implemented.\n",
    "\n",
    "Evaluation metrics used here are accuracy, precision and recall.\n",
    "\n",
    "Using Multinomial Naive Bayes,\n",
    "\n",
    "* Accuracy of prediction is 77.67%.\n",
    "* Precision of prediction is 0.78.\n",
    "* Recall of prediction is 0.77.\n",
    "\n",
    "Using Bernoulli Naive Bayes,\n",
    "\n",
    "* Accuracy of prediction is 77.0%.\n",
    "* Precision of prediction is 0.76.\n",
    "* Recall of prediction is 0.78.\n",
    "\n",
    "Using Logistic Regression,\n",
    "\n",
    "* Accuracy of prediction is 76.67%.\n",
    "* Precision of prediction is 0.8.\n",
    "* Recall of prediction is 0.71.\n",
    "\n",
    "Using Random Forest,\n",
    "\n",
    "* Accuracy of prediction is 74.33%.\n",
    "* Precision of prediction is 0.83.\n",
    "* Recall of prediction is 0.61.\n",
    "\n",
    "Using xgboost,\n",
    "\n",
    "* Accuracy of prediction is 73.33%.\n",
    "* Precision of prediction is 0.79.\n",
    "* Recall of prediction is 0.62.\n",
    "\n",
    "Using SVM,\n",
    "\n",
    "* Accuracy of prediction is 76.67%.\n",
    "* Precision of prediction is 0.8.\n",
    "* Recall of prediction is 0.7.\n",
    "\n",
    "From the above results, Multinomial Naive Bayes is slightly better method compared to Bernoulli Naive Bayes,random forest,svm,xgboost and Logistic Regression, with 77.67% accuracy which means the model built for the prediction of sentiment of the restaurant review gives 77.67% right prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
